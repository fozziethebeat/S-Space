\label{sec:introduction}
Topic models learn bags of related words from large corpora without any
supervision. Based on the words used within a document, they mine topic level
relations by assuming that a single document covers a small set of concise
topics.  Once learned, these topics often correlate well with human concepts.
For example, one model might produce topics that cover ideas such as government
affairs, sports, and movies.  With these unsupervised methods, we can extract 
useful semantic information in a variety of tasks that depend on identifying
unique topics or concepts, such as distributional semantics
\cite{jurgens10sspace}, word sense induction
\cite{vandeCruys11latentWsi,brody09ldawsi}, and information retrieval
\cite{andrzejewski11ldaIR}.  

When using a topic model, we are primarily concerned with the degree to which
the learned topics match human judgments and help us differentiate between
ideas.  But until recently, the evaluation of these models has been ad hoc and
application specific.  Evaluations have ranged from fully automated intrinsic
evaluations to manually crafted extrinsic evaluations.  Previous extrinsic
evaluations have used the learned topics to compactly represent a small fixed
vocabulary and compared this distributional space to human judgments of
similarity \cite{jurgens10sspace}.  But these evaluations are hand constructed
and often costly to perform for domain-specific topics.  Conversely, intrinsic
measures have evaluated the amount of information encoded by the topics, where 
perplexity is one common example\cite{wallach2009evaluation}, however, \newcite{gerrish09tealeaves}
found that these intrinsic measures do not always correlate with semantically
interpretable topics.  Furthermore, few evaluations have used the same metrics
to compare distinct approaches such as Latent Dirichlet Allocation (LDA)
\cite{blei03lda},  Latent Semantic Analysis (LSA) \cite{landauer97solution}, and
Non-negative Matrix Factorization (NMF) \cite{Lee00algorithmsfor}.  This has
made it difficult to know which method is most useful for a given application,
or in terms of extracting useful topics.

We now provide a comprehensive and automated evaluation of these three distinct
models (LDA, LSA, NMF), for automatically learning semantic topics.  While these
models have seen significant improvements, they still represent the core
differences between each approach to modeling topics.  For our evaluation, we
use two recent automated coherence measures \cite{mimno11umass,newman10uci}
originally designed for LDA that bridge the gap between comparisons to
human judgments and intrinsic measures such as perplexity.  We consider several
key questions:
\begin{enumerate*}
{\footnotesize
 \item How many topics should be learned?
 \item How many learned topics are useful?
 \item How do these topics relate to often used semantic tests?
 \item How well do these topics identify similar documents?
 }
\end{enumerate*}

We begin by summarizing the three topic models and highlighting their key
differences.  We then describe the two metrics.  Afterwards, we focus on a
series of experiments that address our four key questions and finally conclude
with some overall remarks.
