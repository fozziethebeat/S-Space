\label{sec:topic-models}

We evaluate three latent factor models that have seen widespread usage:
\begin{enumerate*}
{\small
  \item Latent Dirichlet Allocation
  \item Latent Semantic Analysis with Singular Value Decomposition
  \item Latent Semantic Analysis with Non-negative Matrix Factorization
}
\end{enumerate*}

Each of these models were designed with different goals and are supported
by different statistical theories.  We consider both LSA models as topic models
as they have been used in a variety of similar contexts such as distributional
similarity \cite{jurgens10sspace} and word sense induction
\cite{vandeCruys11latentWsi,brody09ldawsi}.  We evaluate these distinct models
on two shared tasks (1) grouping together similar words while separating
unrelated words and (2) distinguishing between documents focusing on different
concepts.

We distill the different models into a shared representation consisting of two
sets of learned relations: how words interact with topics and how topics
interact with documents.  For a corpus with $\mathcal{D}$ documents and
$\mathcal{V}$ words, we denote these relations in terms of $\mathcal{T}$ topics
as
\begin{description}
\item[(1)] a $\mathcal{V} \times \mathcal{T}$ matrix, $W$, that indicates the strength
each word has in each topic, and
\item[(2)] a $\mathcal{T} \times \mathcal{D}$ matrix, $H$, that indicates the strength each topic
has in each document.  

\end{description}
$\mathcal{T}$ serves as a common parameter to each model.

\subsection{Latent Dirichlet Allocation}

Latent Dirichlet Allocation \cite{blei03lda} learns the relationships
between words, topics, and documents by assuming documents are generated by a
particular probabilistic model.  It first assumes that there are a fixed set of
topics, $\mathcal{T}$ used throughout the corpus, and each topic $z$ is associated with a
multinomial distribution over the vocabulary $\Phi_{z}$, which is drawn from a
Dirichlet prior $Dir(\beta)$.  A given document $D_i$ is then generated by the
following process
\begin{enumerate}
{\small
\item Choose $\Theta_i \sim Dir(\alpha)$, a topic distribution for $D_i$
\item For each word $w_j \in D_i$:
  \begin{enumerate}
  \item Select a topic $z_j \sim \Theta_i$
  \item Select the word $w_j \sim \Phi_{z_j}$
  \end{enumerate}
}
\end{enumerate}
  
In this model, the $\Theta$ distributions represent the probability of each
topic appearing in each document and the $\Phi$ distributions represent the
probability of words being used for each topic.  These two sets of distributions
correspond to our $H$ and $W$ matrices, respectively.  The process above defines
a generative model; given the observed corpus, we use collapsed Gibbs sampling
implementation found in Mallet\footnote{http://mallet.cs.umass.edu/} to infer
the values of the latent variables $\Phi$ and $\Theta$
\cite{griffiths_steyvers04}.  The model relies only on two additional hyper
parameters, $\alpha$ and $\beta$,  that guide the distributions.

\begin{table*}[h!t!b!]
\footnotesize
\center
\begin{tabular}{|cllcc|}
\multicolumn{1}{c}{Model} & Label & Top Words & UMass & \multicolumn{1}{l}{UCI} \\
\hline
\multicolumn{3}{l}{\textbf{High Quality Topics}} \\
\hline
\multirow{2}{*}{LDA} 
% 500-32
& interview & told asked wanted interview people made thought time called knew 
& -2.52 & 1.29 \\
% 500-176
& wine & wine wines bottle grapes made winery cabernet grape pinot red 
& -1.97 & 1.30 \\
\hline
\multirow{2}{*}{NMF} 
% 500-144
& grilling & grilled sweet spicy fried pork dish shrimp menu dishes sauce 
& -1.01 & 1.98 \\
% 500-120
& cloning & embryonic cloned embryo human research stem embryos cell cloning cells
& -1.84 & 1.46 \\
\hline
\multirow{2}{*}{SVD} 
% 500-5
& cooking & sauce food restaurant water oil salt chicken pepper wine cup 
& -1.87 & -1.21 \\
% 500-25
& stocks & fund funds investors weapons stocks mutual stock movie film show 
& -2.30 & -1.88 \\
\hline

\multicolumn{3}{l}{\textbf{Low Quality Topics}} \\
\hline
\multirow{2}{*}{LDA} 
% 500-290
& rates & 10-yr rate 3-month percent 6-month bds bd 30-yr funds robot 
& -1.94 & -12.32 \\
% 500-340
& charity & fund contributions .com family apartment charities rent 22d children assistance
& -2.43 & -8.88 \\
\hline
\multirow{2}{*}{NMF} 
% 500-76
& plants & stem fruitful stems trunk fruiting currants branches fence currant espalier 
& -3.12 & -12.59 \\
% 500-33
& farming & buzzards groundhog prune hoof pruned pruning vines wheelbarrow tree clematis
& -1.90 & -12.56 \\
\hline
\multirow{2}{*}{SVD} 
% 500-7
& city & building city area buildings p.m. floors house listed eat-in a.m.
& -2.70 & -8.03 \\
% 500-160
& time & p.m. system study a.m. office political found school night yesterday 
& -1.67 & -7.02 \\
\hline
\end{tabular}
\caption{Top 10 words from several high and low quality topics when ordered by
the UCI Coherence Measure.  Topic labels were chosen in an ad hoc manner only to
briefly summarize the topic's focus.}
\label{tab:best}
\end{table*}


\subsection{Latent Semantic Analysis}

Latent Semantic Analysis \cite{landauer97solution,landauer98lsa} learns topics
by first forming a traditional term by document matrix used in information
retrieval and then smoothing the counts to enhance the weight of informative
words.  Based on the original LSA model, we use the Log-Entropy transform.  
LSA then decomposes this smoothed, term by document matrix in order to
generalize observed relations between words and documents.  For both LSA models,
we used implementations found in the S-Space
package.\footnote{https://github.com/fozziethebeat/S-Space}

Traditionally, LSA has used the Singular Value Decomposition, but we also
consider Non-negative Matrix Factorization as we've seen NMF applied in similar
situations \cite{pauca04nmfIR} and others have found a connection between NMF
and Probabilistic Latent Semantic Analysis \cite{ding08nmfPlsa}, an extension to
LSA.  We later refer to these two LSA models simply as SVD and NMF to emphasize
the difference in factorization method.

\paragraph{Singular Value Decomposition}
decomposes $M$ into three smaller matrices
$$
M=U \Sigma V^{T}
$$
\noindent
and minimizes Frobenius norm of $M$'s reconstruction error with the constraint
that the rows of $U$ and $V$ are orthonormal eigenvectors.  Interestingly, the
decomposition is agnostic to the number of desired dimensions.  Instead, the
rows and columns in $U$ and $V^T$ are ordered based on their descriptive power,
i.e. how well they remove noise, which is encoded by the diagonal singular value
matrix $\Sigma$.  As such, reduction is done by retaining the first
$\mathcal{T}$ rows and columns from $U$ and $V^T$.  For our generalization, we
use $W=U \Sigma$ and $H = \Sigma V^T$.  We note that values in $U$ and $V^T$ can
be both negative and positive, preventing a straightforward interpretation as
unnormalized probabilities

\paragraph{Non-negative Matrix Factorization}
also factorizes $M$ by minimizing the reconstruction error, but with only one
constraint: the decomposed matrices consist of only non-negative values.
In this respect, we can consider it to be
learning an unnormalized probability distributions over topics.  We use the original
Euclidean least squares definition of NMF\footnote{We note that the
alternative KL-Divergence form of NMF has been directly linked to PLSA
\cite{ding08nmfPlsa}}.  Formally, NMF is defined as

$$M = W H$$
\noindent
where $H$ and $W$ map directly onto our generalization.  As in the original NMF
work, we learn these unnormalized probabilities by initializing each set of
probabilities at random and update them according to the following iterative
update rules

$$
\begin{array}{cc}
  W = W \frac{M H^T}{W H H^T} &  H = H \frac{W^T M}{W^T W H} \\
\end{array}
$$
